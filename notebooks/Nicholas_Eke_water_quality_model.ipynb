{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORMATIVE ASSIGNMENT II: WATER QUALITY MODEL\n",
    "\n",
    "## 1. Introduction\n",
    "**Assignment**: Building a Classification Model Using Neural Networks\n",
    "\n",
    "**Objective:**\n",
    "Develop a neural network-based classification model using a provided dataset, incorporating multiple optimization techniques and ensuring equitable group contribution.\n",
    "\n",
    "**In this notebook, we will take the cleaned and imputed dataset and use it to train, test, and evaluate a deep learning model**:\n",
    "\n",
    "The key steps we'll cover are:\n",
    "1. Loading the preprocessed (imputed) dataset.\n",
    "2. Separating features and the target variable.\n",
    "3. Splitting the dataset into three distinct portions: training, validation, and testing sets. This is crucial for robust model development and evaluation.\n",
    "4. Applying feature scaling (StandardScaler) correctly after the split to prevent data leakage.\n",
    "\n",
    "**Note:** The data cleaning and imputation steps were performed in a previous notebook. \n",
    "If you'd like to review that process, please refer to: [Data Preprocessing Notebook](data_preprocessing.ipynb).\n",
    "\n",
    "**Model Details**\n",
    "\n",
    "\n",
    "| Engineer Name     | Regularizer | Optimizer | Early Stopping  | Dropout Rate | Learning Rate |\n",
    "| ----------------- | ---------------------------- | --------- | ------------------------------------------------ | ------------ | ------------- |\n",
    "| Nicholas Eke | L1 (0.01)           | RMSprop      | val_loss, p = 5                    | 0.3         | 0.0005 rate        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling\n",
    "### 2.1 Declaring Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.22.4\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: numpy, tzdata, threadpoolctl, scipy, pytz, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.5.1 numpy-2.2.6 pandas-2.3.0 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages if needed\n",
    "%pip install pandas scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If file_path is not a string.\n",
    "        FileNotFoundError: If the file specified by file_path is not found.\n",
    "        Exception: For other pandas-related read errors.\n",
    "    \"\"\"\n",
    "    if not isinstance(file_path, str):\n",
    "        raise TypeError(\"file_path must be a string.\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading CSV file '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset \n",
    "# We verify the columns, data types, and ensure that our imputation worked \n",
    "# All columns should have non-null counts matching the total number of entries.\n",
    "def display_initial_info(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Displays the first few rows and basic info of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to inspect.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If df is not a pandas DataFrame.\n",
    "        ValueError: If df is None.\n",
    "    \"\"\"\n",
    "    print(\"===============First 5 rows:===============\")\n",
    "    print(df.head())\n",
    "    print(\"\\n===============Information:===============\")\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_features_target(df: pd.DataFrame, target_column: str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Separates features and the target variable from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to separate.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.Series]: A tuple containing features (X) and target (y).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If df is not a DataFrame or target_column is not a string.\n",
    "        ValueError: If df is None or target_column is not found in df.columns.\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    print(f\"\\nFeatures (X) and target (y, column: '{target_column}') have been separated.\")\n",
    "    print(f\"Shape of features (X): {X.shape}\")\n",
    "    print(f\"Shape of target (y): {y.shape}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X: pd.DataFrame, y: pd.Series,\n",
    "               test_size: float = 0.15,\n",
    "               val_relative_to_train_val_size: float = 0.15 / 0.85,\n",
    "               random_state: int = 42,\n",
    "               stratify_data: bool = True) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Splits feature and target data into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features.\n",
    "        y (pd.Series): Target variable.\n",
    "        test_size (float): Proportion of the dataset to allocate to the test set.\n",
    "        val_relative_to_train_val_size (float): Proportion of the (train+validation) set to allocate to validation.\n",
    "        random_state (int): Seed for random number generator for reproducibility.\n",
    "        stratify_data (bool): Whether to stratify the split based on the target variable y.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If X is not a DataFrame or y is not a Series.\n",
    "        ValueError: If X or y is None, or if shapes are incompatible.\n",
    "    \"\"\"\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Shape mismatch: X has {len(X)} samples, y has {len(y)} samples.\")\n",
    "\n",
    "    stratify_option_main = y if stratify_data else None\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=stratify_option_main\n",
    "    )\n",
    "\n",
    "    stratify_option_tv = y_train_val if stratify_data else None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_relative_to_train_val_size,\n",
    "        random_state=random_state, stratify=stratify_option_tv\n",
    "    )\n",
    "\n",
    "    print(\"\\nData splitting completed.\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame) -> tuple[StandardScaler, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fits StandardScaler on X_train and transforms X_train, X_val, X_test.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_val (pd.DataFrame): Validation features.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The fitted scaler object, and the scaled DataFrames (X_train_scaled, X_val_scaled, X_test_scaled).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If any input is not a pandas DataFrame.\n",
    "        ValueError: If any input DataFrame is None.\n",
    "    \"\"\"\n",
    "    for df_name, df_obj in [(\"X_train\", X_train), (\"X_val\", X_val), (\"X_test\", X_test)]:\n",
    "        if not isinstance(df_obj, pd.DataFrame):\n",
    "            raise TypeError(f\"Input '{df_name}' must be a pandas DataFrame. Got {type(df_obj)}.\")\n",
    "        if df_obj is None:\n",
    "            raise ValueError(f\"Input DataFrame '{df_name}' cannot be None.\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    print(\"\\nFitting StandardScaler on X_train...\")\n",
    "    # Ensure X_train has data (not empty) before fitting\n",
    "    if X_train.empty:\n",
    "        raise ValueError(\"X_train is empty, cannot fit StandardScaler.\")\n",
    "    scaler.fit(X_train) # Fit ONLY on training data\n",
    "    print(\"Scaler fitted.\")\n",
    "\n",
    "    print(\"Transforming X_train, X_val, and X_test...\")\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_val)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    print(\"Transformation complete.\")\n",
    "\n",
    "    # Convert back to DataFrames\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=X_train.columns, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=X_val.columns, index=X_val.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=X_test.columns, index=X_test.index)\n",
    "    print(\"Scaled data converted back to DataFrames.\")\n",
    "\n",
    "    return scaler, X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Scaling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset from: ../data/imputed_water_potability_data.csv\n",
      "===============First 5 rows:===============\n",
      "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
      "0  7.036752  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
      "1  3.716080  129.422921  18630.057858     6.635246  333.073546    592.885359   \n",
      "2  8.099124  224.236259  19909.541732     9.275884  333.073546    418.606213   \n",
      "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
      "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
      "\n",
      "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0       10.379783        86.990970   2.963135           0  \n",
      "1       15.180013        56.329076   4.500656           0  \n",
      "2       16.868637        66.420093   3.055934           0  \n",
      "3       18.436524       100.341674   4.628771           0  \n",
      "4       11.558279        31.997993   4.075075           0  \n",
      "\n",
      "===============Information:===============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               3276 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          3276 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3276 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Here, we load the dataset that has already undergone cleaning and imputation.\n",
    "# It's important to use the version where missing values have already been handled.\n",
    "# Path to the file which has already missing fields handled\n",
    "FILE_PATH_IMPUTED = \"../data/imputed_water_potability_data.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data(FILE_PATH_IMPUTED)\n",
    "\n",
    "# Display the overview of the dataset\n",
    "display_initial_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features (X) and target (y, column: 'Potability') have been separated.\n",
      "Shape of features (X): (3276, 9)\n",
      "Shape of target (y): (3276,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "X, y = separate_features_target(df, \"Potability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data splitting completed.\n",
      "Shape of X_train: (2292, 9), y_train: (2292,)\n",
      "Shape of X_val: (492, 9), y_val: (492,)\n",
      "Shape of X_test: (492, 9), y_test: (492,)\n",
      "\n",
      "Proportion of samples in each set (approximate):\n",
      "Training set: 69.96%\n",
      "Validation set: 15.02%\n",
      "Test set: 15.02%\n"
     ]
    }
   ],
   "source": [
    "# Initialize split variables to None\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = [None] * 6\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, stratify_data=True)\n",
    "\n",
    "        # Display proportions\n",
    "        print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"Shape of X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        total_samples = len(X)\n",
    "        print(\"\\nProportion of samples in each set (approximate):\")\n",
    "        print(f\"Training set: {len(X_train)/total_samples*100:.2f}%\")\n",
    "        print(f\"Validation set: {len(X_val)/total_samples*100:.2f}%\")\n",
    "        print(f\"Test set: {len(X_test)/total_samples*100:.2f}%\")\n",
    "\n",
    "    except TypeError as e:\n",
    "        raise ValueError(f\"TypeError during data splitting: {e}\")\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"ValueError during data splitting: {e}\")\n",
    "else:\n",
    "    raise ValueError(\"X or y is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting StandardScaler on X_train...\n",
      "Scaler fitted.\n",
      "Transforming X_train, X_val, and X_test...\n",
      "Transformation complete.\n",
      "Scaled data converted back to DataFrames.\n",
      "\n",
      "--- Verification of Scaled Training Data ---\n",
      "===============First 5 rows:===============\n",
      "            ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
      "2766  0.292120  0.027566  0.522327     0.038376 -1.781081     -0.634797   \n",
      "2505 -0.104021  0.816164 -0.481655     0.540023 -0.954435      0.033630   \n",
      "163  -0.647064 -1.665957 -1.420733     0.308547 -1.555323     -0.140706   \n",
      "43    1.961394  0.193955 -1.379953    -0.160524  0.108557     -1.163968   \n",
      "2040 -0.022284  1.805507 -0.796542     0.846080  0.798238      0.565539   \n",
      "\n",
      "      Organic_carbon  Trihalomethanes  Turbidity  \n",
      "2766       -0.341967         0.209553  -0.180315  \n",
      "2505       -0.619904         0.161566  -1.631684  \n",
      "163         0.672902        -0.993148   0.866453  \n",
      "43          2.973166         0.352298   0.939613  \n",
      "2040        0.099535        -1.655482  -0.941722  \n",
      "\n",
      "===============Information:===============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2292 entries, 2766 to 1559\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2292 non-null   float64\n",
      " 1   Hardness         2292 non-null   float64\n",
      " 2   Solids           2292 non-null   float64\n",
      " 3   Chloramines      2292 non-null   float64\n",
      " 4   Sulfate          2292 non-null   float64\n",
      " 5   Conductivity     2292 non-null   float64\n",
      " 6   Organic_carbon   2292 non-null   float64\n",
      " 7   Trihalomethanes  2292 non-null   float64\n",
      " 8   Turbidity        2292 non-null   float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 179.1 KB\n",
      "\n",
      "Descriptive statistics of X_train_scaled (mean ~0, std ~1):\n",
      "            ph  Hardness   Solids  Chloramines  Sulfate  Conductivity  \\\n",
      "count  2292.00   2292.00  2292.00      2292.00  2292.00       2292.00   \n",
      "mean     -0.00      0.00     0.00        -0.00     0.00         -0.00   \n",
      "std       1.00      1.00     1.00         1.00     1.00          1.00   \n",
      "min      -4.19     -4.52    -2.48        -3.43    -4.29         -3.06   \n",
      "25%      -0.56     -0.58    -0.71        -0.63    -0.46         -0.75   \n",
      "50%      -0.02      0.02    -0.11         0.01    -0.01         -0.05   \n",
      "75%       0.54      0.61     0.60         0.62     0.44          0.69   \n",
      "max       4.45      3.84     4.46         3.72     4.13          4.09   \n",
      "\n",
      "       Organic_carbon  Trihalomethanes  Turbidity  \n",
      "count         2292.00          2292.00    2292.00  \n",
      "mean             0.00             0.00       0.00  \n",
      "std              1.00             1.00       1.00  \n",
      "min             -3.04            -3.70      -3.17  \n",
      "25%             -0.67            -0.62      -0.67  \n",
      "50%             -0.02             0.02      -0.00  \n",
      "75%              0.69             0.64       0.67  \n",
      "max              3.92             3.67       3.57  \n",
      "\n",
      "Workflow complete. Data is prepared for model training.\n",
      "Prepared data sets:\n",
      "X_train_scaled: (2292, 9), y_train: (2292,)\n",
      "X_val_scaled: (492, 9), y_val: (492,)\n",
      "X_test_scaled: (492, 9), y_test: (492,)\n"
     ]
    }
   ],
   "source": [
    "scaler_object, X_train_scaled, X_val_scaled, X_test_scaled = [None] * 4\n",
    "\n",
    "if X_train is not None and X_val is not None and X_test is not None:\n",
    "    try:\n",
    "        scaler_object, X_train_scaled, X_val_scaled, X_test_scaled = scale_features(X_train, X_val, X_test)\n",
    "\n",
    "        print(\"\\n--- Verification of Scaled Training Data ---\")\n",
    "        display_initial_info(X_train_scaled)\n",
    "\n",
    "        print(\"\\nDescriptive statistics of X_train_scaled (mean ~0, std ~1):\")\n",
    "        print(X_train_scaled.describe().round(2))\n",
    "\n",
    "        print(\"\\nWorkflow complete. Data is prepared for model training.\")\n",
    "        print(\"Prepared data sets:\")\n",
    "        print(f\"X_train_scaled: {X_train_scaled.shape}, y_train: {y_train.shape if y_train is not None else 'N/A'}\")\n",
    "        print(f\"X_val_scaled: {X_val_scaled.shape}, y_val: {y_val.shape if y_val is not None else 'N/A'}\")\n",
    "        print(f\"X_test_scaled: {X_test_scaled.shape}, y_test: {y_test.shape if y_test is not None else 'N/A'}\")\n",
    "\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error during feature scaling: {e}\")\n",
    "else:\n",
    "    print(\"Skipping feature scaling as data splits are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and Training a Model\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "Collecting tensorboard~=2.19.0\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (25.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras>=3.5.0\n",
      "  Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python310\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting h5py>=3.11.0\n",
      "  Using cached h5py-3.13.0-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.72.1-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1\n",
      "  Using cached ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl (209 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.1.3)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Collecting namex\n",
      "  Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Collecting optree\n",
      "  Using cached optree-0.16.0-cp310-cp310-win_amd64.whl (304 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nicholas eke\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: MarkupSafe, markdown-it-py, wheel, werkzeug, urllib3, tensorboard-data-server, rich, protobuf, optree, namex, ml-dtypes, markdown, idna, h5py, grpcio, charset-normalizer, certifi, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorboard, requests, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.3.0 astunparse-1.6.3 certifi-2025.4.26 charset-normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.72.1 h5py-3.13.0 idna-3.10 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 requests-2.32.3 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas Eke\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
