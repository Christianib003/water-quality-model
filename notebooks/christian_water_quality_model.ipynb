{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORMATIVE ASSIGNMENT II: WATER QUALITY MODEL\n",
    "\n",
    "## 1. Introduction\n",
    "**Assignment**: Building a Classification Model Using Neural Networks\n",
    "\n",
    "**Objective:**\n",
    "Develop a neural network-based classification model using a provided dataset, incorporating multiple optimization techniques and ensuring equitable group contribution.\n",
    "\n",
    "**In this notebook, we will take the cleaned and imputed dataset and use it to train, test, and evaluate a deep learning model**:\n",
    "\n",
    "The key steps we'll cover are:\n",
    "1. Loading the preprocessed (imputed) dataset.\n",
    "2. Separating features and the target variable.\n",
    "3. Splitting the dataset into three distinct portions: training, validation, and testing sets. This is crucial for robust model development and evaluation.\n",
    "4. Applying feature scaling (StandardScaler) correctly after the split to prevent data leakage.\n",
    "\n",
    "**Note:** The data cleaning and imputation steps were performed in a previous notebook. \n",
    "If you'd like to review that process, please refer to: [Data Preprocessing Notebook](data_preprocessing.ipynb).\n",
    "\n",
    "**Model Details**\n",
    "\n",
    "\n",
    "| Engineer Name     | Regularizer | Optimizer | Early Stopping  | Dropout Rate | Learning Rate |\n",
    "| ----------------- | ---------------------------- | --------- | ------------------------------------------------ | ------------ | ------------- |\n",
    "| Christian Iradukunda B. | Your Regularizer           | Nadam      | Your early stopping                    | your dropout rate         | Your Learning rate        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling\n",
    "### 2.1 Declaring Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/testsolutions/Documents/school/year3/term1/water-quality-model/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If file_path is not a string.\n",
    "        FileNotFoundError: If the file specified by file_path is not found.\n",
    "        Exception: For other pandas-related read errors.\n",
    "    \"\"\"\n",
    "    if not isinstance(file_path, str):\n",
    "        raise TypeError(\"file_path must be a string.\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading CSV file '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset \n",
    "# We verify the columns, data types, and ensure that our imputation worked \n",
    "# All columns should have non-null counts matching the total number of entries.\n",
    "def display_initial_info(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Displays the first few rows and basic info of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to inspect.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If df is not a pandas DataFrame.\n",
    "        ValueError: If df is None.\n",
    "    \"\"\"\n",
    "    print(\"===============First 5 rows:===============\")\n",
    "    print(df.head())\n",
    "    print(\"\\n===============Information:===============\")\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_features_target(df: pd.DataFrame, target_column: str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Separates features and the target variable from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to separate.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.Series]: A tuple containing features (X) and target (y).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If df is not a DataFrame or target_column is not a string.\n",
    "        ValueError: If df is None or target_column is not found in df.columns.\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    print(f\"\\nFeatures (X) and target (y, column: '{target_column}') have been separated.\")\n",
    "    print(f\"Shape of features (X): {X.shape}\")\n",
    "    print(f\"Shape of target (y): {y.shape}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X: pd.DataFrame, y: pd.Series,\n",
    "               test_size: float = 0.15,\n",
    "               val_relative_to_train_val_size: float = 0.15 / 0.85,\n",
    "               random_state: int = 42,\n",
    "               stratify_data: bool = True) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Splits feature and target data into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features.\n",
    "        y (pd.Series): Target variable.\n",
    "        test_size (float): Proportion of the dataset to allocate to the test set.\n",
    "        val_relative_to_train_val_size (float): Proportion of the (train+validation) set to allocate to validation.\n",
    "        random_state (int): Seed for random number generator for reproducibility.\n",
    "        stratify_data (bool): Whether to stratify the split based on the target variable y.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If X is not a DataFrame or y is not a Series.\n",
    "        ValueError: If X or y is None, or if shapes are incompatible.\n",
    "    \"\"\"\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Shape mismatch: X has {len(X)} samples, y has {len(y)} samples.\")\n",
    "\n",
    "    stratify_option_main = y if stratify_data else None\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=stratify_option_main\n",
    "    )\n",
    "\n",
    "    stratify_option_tv = y_train_val if stratify_data else None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_relative_to_train_val_size,\n",
    "        random_state=random_state, stratify=stratify_option_tv\n",
    "    )\n",
    "\n",
    "    print(\"\\nData splitting completed.\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame) -> tuple[StandardScaler, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fits StandardScaler on X_train and transforms X_train, X_val, X_test.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_val (pd.DataFrame): Validation features.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The fitted scaler object, and the scaled DataFrames (X_train_scaled, X_val_scaled, X_test_scaled).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If any input is not a pandas DataFrame.\n",
    "        ValueError: If any input DataFrame is None.\n",
    "    \"\"\"\n",
    "    for df_name, df_obj in [(\"X_train\", X_train), (\"X_val\", X_val), (\"X_test\", X_test)]:\n",
    "        if not isinstance(df_obj, pd.DataFrame):\n",
    "            raise TypeError(f\"Input '{df_name}' must be a pandas DataFrame. Got {type(df_obj)}.\")\n",
    "        if df_obj is None:\n",
    "            raise ValueError(f\"Input DataFrame '{df_name}' cannot be None.\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    print(\"\\nFitting StandardScaler on X_train...\")\n",
    "    # Ensure X_train has data (not empty) before fitting\n",
    "    if X_train.empty:\n",
    "        raise ValueError(\"X_train is empty, cannot fit StandardScaler.\")\n",
    "    scaler.fit(X_train) # Fit ONLY on training data\n",
    "    print(\"Scaler fitted.\")\n",
    "\n",
    "    print(\"Transforming X_train, X_val, and X_test...\")\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_val)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    print(\"Transformation complete.\")\n",
    "\n",
    "    # Convert back to DataFrames\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=X_train.columns, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=X_val.columns, index=X_val.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=X_test.columns, index=X_test.index)\n",
    "    print(\"Scaled data converted back to DataFrames.\")\n",
    "\n",
    "    return scaler, X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Scaling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset from: ../data/imputed_water_potability_data.csv\n",
      "===============First 5 rows:===============\n",
      "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
      "0  7.036752  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
      "1  3.716080  129.422921  18630.057858     6.635246  333.073546    592.885359   \n",
      "2  8.099124  224.236259  19909.541732     9.275884  333.073546    418.606213   \n",
      "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
      "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
      "\n",
      "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0       10.379783        86.990970   2.963135           0  \n",
      "1       15.180013        56.329076   4.500656           0  \n",
      "2       16.868637        66.420093   3.055934           0  \n",
      "3       18.436524       100.341674   4.628771           0  \n",
      "4       11.558279        31.997993   4.075075           0  \n",
      "\n",
      "===============Information:===============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               3276 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          3276 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3276 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Here, we load the dataset that has already undergone cleaning and imputation.\n",
    "# It's important to use the version where missing values have already been handled.\n",
    "# Path to the file which has already missing fields handled\n",
    "FILE_PATH_IMPUTED = \"../data/imputed_water_potability_data.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data(FILE_PATH_IMPUTED)\n",
    "\n",
    "# Display the overview of the dataset\n",
    "display_initial_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features (X) and target (y, column: 'Potability') have been separated.\n",
      "Shape of features (X): (3276, 9)\n",
      "Shape of target (y): (3276,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "X, y = separate_features_target(df, \"Potability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data splitting completed.\n",
      "Shape of X_train: (2292, 9), y_train: (2292,)\n",
      "Shape of X_val: (492, 9), y_val: (492,)\n",
      "Shape of X_test: (492, 9), y_test: (492,)\n",
      "\n",
      "Proportion of samples in each set (approximate):\n",
      "Training set: 69.96%\n",
      "Validation set: 15.02%\n",
      "Test set: 15.02%\n"
     ]
    }
   ],
   "source": [
    "# Initialize split variables to None\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = [None] * 6\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, stratify_data=True)\n",
    "\n",
    "        # Display proportions\n",
    "        print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"Shape of X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        total_samples = len(X)\n",
    "        print(\"\\nProportion of samples in each set (approximate):\")\n",
    "        print(f\"Training set: {len(X_train)/total_samples*100:.2f}%\")\n",
    "        print(f\"Validation set: {len(X_val)/total_samples*100:.2f}%\")\n",
    "        print(f\"Test set: {len(X_test)/total_samples*100:.2f}%\")\n",
    "\n",
    "    except TypeError as e:\n",
    "        raise ValueError(f\"TypeError during data splitting: {e}\")\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"ValueError during data splitting: {e}\")\n",
    "else:\n",
    "    raise ValueError(\"X or y is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting StandardScaler on X_train...\n",
      "Scaler fitted.\n",
      "Transforming X_train, X_val, and X_test...\n",
      "Transformation complete.\n",
      "Scaled data converted back to DataFrames.\n",
      "\n",
      "--- Verification of Scaled Training Data ---\n",
      "===============First 5 rows:===============\n",
      "            ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
      "2766  0.292120  0.027566  0.522327     0.038376 -1.781081     -0.634797   \n",
      "2505 -0.104021  0.816164 -0.481655     0.540023 -0.954435      0.033630   \n",
      "163  -0.647064 -1.665957 -1.420733     0.308547 -1.555323     -0.140706   \n",
      "43    1.961394  0.193955 -1.379953    -0.160524  0.108557     -1.163968   \n",
      "2040 -0.022284  1.805507 -0.796542     0.846080  0.798238      0.565539   \n",
      "\n",
      "      Organic_carbon  Trihalomethanes  Turbidity  \n",
      "2766       -0.341967         0.209553  -0.180315  \n",
      "2505       -0.619904         0.161566  -1.631684  \n",
      "163         0.672902        -0.993148   0.866453  \n",
      "43          2.973166         0.352298   0.939613  \n",
      "2040        0.099535        -1.655482  -0.941722  \n",
      "\n",
      "===============Information:===============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2292 entries, 2766 to 1559\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2292 non-null   float64\n",
      " 1   Hardness         2292 non-null   float64\n",
      " 2   Solids           2292 non-null   float64\n",
      " 3   Chloramines      2292 non-null   float64\n",
      " 4   Sulfate          2292 non-null   float64\n",
      " 5   Conductivity     2292 non-null   float64\n",
      " 6   Organic_carbon   2292 non-null   float64\n",
      " 7   Trihalomethanes  2292 non-null   float64\n",
      " 8   Turbidity        2292 non-null   float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 179.1 KB\n",
      "\n",
      "Descriptive statistics of X_train_scaled (mean ~0, std ~1):\n",
      "            ph  Hardness   Solids  Chloramines  Sulfate  Conductivity  \\\n",
      "count  2292.00   2292.00  2292.00      2292.00  2292.00       2292.00   \n",
      "mean     -0.00      0.00     0.00        -0.00     0.00         -0.00   \n",
      "std       1.00      1.00     1.00         1.00     1.00          1.00   \n",
      "min      -4.19     -4.52    -2.48        -3.43    -4.29         -3.06   \n",
      "25%      -0.56     -0.58    -0.71        -0.63    -0.46         -0.75   \n",
      "50%      -0.02      0.02    -0.11         0.01    -0.01         -0.05   \n",
      "75%       0.54      0.61     0.60         0.62     0.44          0.69   \n",
      "max       4.45      3.84     4.46         3.72     4.13          4.09   \n",
      "\n",
      "       Organic_carbon  Trihalomethanes  Turbidity  \n",
      "count         2292.00          2292.00    2292.00  \n",
      "mean             0.00             0.00       0.00  \n",
      "std              1.00             1.00       1.00  \n",
      "min             -3.04            -3.70      -3.17  \n",
      "25%             -0.67            -0.62      -0.67  \n",
      "50%             -0.02             0.02      -0.00  \n",
      "75%              0.69             0.64       0.67  \n",
      "max              3.92             3.67       3.57  \n",
      "\n",
      "Workflow complete. Data is prepared for model training.\n",
      "Prepared data sets:\n",
      "X_train_scaled: (2292, 9), y_train: (2292,)\n",
      "X_val_scaled: (492, 9), y_val: (492,)\n",
      "X_test_scaled: (492, 9), y_test: (492,)\n"
     ]
    }
   ],
   "source": [
    "scaler_object, X_train_scaled, X_val_scaled, X_test_scaled = [None] * 4\n",
    "\n",
    "if X_train is not None and X_val is not None and X_test is not None:\n",
    "    try:\n",
    "        scaler_object, X_train_scaled, X_val_scaled, X_test_scaled = scale_features(X_train, X_val, X_test)\n",
    "\n",
    "        print(\"\\n--- Verification of Scaled Training Data ---\")\n",
    "        display_initial_info(X_train_scaled)\n",
    "\n",
    "        print(\"\\nDescriptive statistics of X_train_scaled (mean ~0, std ~1):\")\n",
    "        print(X_train_scaled.describe().round(2))\n",
    "\n",
    "        print(\"\\nWorkflow complete. Data is prepared for model training.\")\n",
    "        print(\"Prepared data sets:\")\n",
    "        print(f\"X_train_scaled: {X_train_scaled.shape}, y_train: {y_train.shape if y_train is not None else 'N/A'}\")\n",
    "        print(f\"X_val_scaled: {X_val_scaled.shape}, y_val: {y_val.shape if y_val is not None else 'N/A'}\")\n",
    "        print(f\"X_test_scaled: {X_test_scaled.shape}, y_test: {y_test.shape if y_test is not None else 'N/A'}\")\n",
    "\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error during feature scaling: {e}\")\n",
    "else:\n",
    "    print(\"Skipping feature scaling as data splits are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and Training a Model\n",
    "\n",
    "### 3.1. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defnine a function to create a Keras Sequential model with specified hyperparameters\n",
    "def create_model(input_shape: tuple,\n",
    "                 dropout_rate: float = 0.15,\n",
    "                 l1_reg: float = 0.01,\n",
    "                 l2_reg: float = 0.01) -> Sequential:\n",
    "    \"\"\"\n",
    "    Creates and returns a Keras Sequential model with the specified hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (number of features,).\n",
    "        dropout_rate (float): The rate for the Dropout layer.\n",
    "        l1_reg (float): The factor for the L1 activity regularizer.\n",
    "        l2_reg (float): The factor for the L2 kernel regularizer.\n",
    "\n",
    "    Returns:\n",
    "        Sequential: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    # Initialize the regularizers\n",
    "    kernel_regularizer = l2(l2_reg)\n",
    "    activity_regularizer = l1(l1_reg)\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        # Input Layer and First Hidden Layer\n",
    "        Dense(64, activation='relu', input_shape=input_shape, name='hidden_layer_1'),\n",
    "\n",
    "        # Second Hidden Layer with specified Regularizers\n",
    "        Dense(128, activation='relu',\n",
    "              kernel_regularizer=kernel_regularizer,\n",
    "              activity_regularizer=activity_regularizer,\n",
    "              name='hidden_layer_2_with_regularizers'),\n",
    "        \n",
    "        # Dropout Layer to prevent overfitting\n",
    "        Dropout(dropout_rate, name='dropout_layer'),\n",
    "\n",
    "        # Third Hidden Layer\n",
    "        Dense(64, activation='relu', name='hidden_layer_3'),\n",
    "\n",
    "        # Output Layer\n",
    "        Dense(1, activation='sigmoid', name='output_layer')\n",
    "    ])\n",
    "\n",
    "    print(\"\\nModel architecture created successfully.\")\n",
    "    model.summary() # Print a summary of the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defnine a function to train the model\n",
    "def train_model(model: Sequential,\n",
    "                X_train: pd.DataFrame,\n",
    "                y_train: pd.Series,\n",
    "                X_val: pd.DataFrame,\n",
    "                y_val: pd.Series,\n",
    "                learning_rate: float = 0.06,\n",
    "                patience: int = 8,\n",
    "                epochs: int = 150,\n",
    "                batch_size: int = 32) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Compiles and trains the Keras model.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): The Keras model to train.\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels.\n",
    "        learning_rate (float): The learning rate for the Nadam optimizer.\n",
    "        patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "        epochs (int): The maximum number of epochs to train for.\n",
    "        batch_size (int): The number of samples per gradient update.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: Object containing the history of the training process.\n",
    "    \"\"\"\n",
    "    # 1. Set up the EarlyStopping callback\n",
    "    # We monitor 'val_loss' and stop if it doesn't improve for 'patience' epochs.\n",
    "    # restore_best_weights=True ensures the model weights are reset to the best ones found.\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # 2. Compile the model\n",
    "    # We use the Nadam optimizer with the specified learning rate.\n",
    "    model.compile(\n",
    "        optimizer=Nadam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')] # Tracking accuracy and AUC\n",
    "    )\n",
    "    print(\"\\nModel compiled successfully.\")\n",
    "\n",
    "    # 3. Train (fit) the model\n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        verbose=1 # Set to 1 to see progress bar, 2 for one line per epoch, 0 for silent\n",
    "    )\n",
    "    print(\"Model training complete.\")\n",
    "    return history\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
