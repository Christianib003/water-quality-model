{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christianib003/water-quality-model/blob/Nicholas/Nicholas_Eke_water_quality_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1447U2j6_4N4"
      },
      "source": [
        "# FORMATIVE ASSIGNMENT II: WATER QUALITY MODEL\n",
        "\n",
        "## 1. Introduction\n",
        "**Assignment**: Building a Classification Model Using Neural Networks\n",
        "\n",
        "**Objective:**\n",
        "Develop a neural network-based classification model using a provided dataset, incorporating multiple optimization techniques and ensuring equitable group contribution.\n",
        "\n",
        "**In this notebook, we will take the cleaned and imputed dataset and use it to train, test, and evaluate a deep learning model**:\n",
        "\n",
        "The key steps we'll cover are:\n",
        "1. Loading the preprocessed (imputed) dataset.\n",
        "2. Separating features and the target variable.\n",
        "3. Splitting the dataset into three distinct portions: training, validation, and testing sets. This is crucial for robust model development and evaluation.\n",
        "4. Applying feature scaling (StandardScaler) correctly after the split to prevent data leakage.\n",
        "\n",
        "**Note:** The data cleaning and imputation steps were performed in a previous notebook.\n",
        "If you'd like to review that process, please refer to: [Data Preprocessing Notebook](data_preprocessing.ipynb).\n",
        "\n",
        "**Model Details**\n",
        "\n",
        "\n",
        "| Engineer Name     | Regularizer | Optimizer | Early Stopping  | Dropout Rate | Learning Rate |\n",
        "| ----------------- | ---------------------------- | --------- | ------------------------------------------------ | ------------ | ------------- |\n",
        "| Nicholas Eke | L1 (0.01)           | RMSprop      | val_loss, p = 5                    | 0.3         | 0.0005 rate        |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONzKzcDg_4N-"
      },
      "source": [
        "## 2. Feature Scaling\n",
        "### 2.1 Declaring Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRLc3Fzu_4N_",
        "outputId": "5f948f1b-b953-4626-8b1d-19d5264a152a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install missing packages if needed\n",
        "%pip install pandas scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Clvk-ZnH_4OF"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads a CSV file into a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If file_path is not a string.\n",
        "        FileNotFoundError: If the file specified by file_path is not found.\n",
        "        Exception: For other pandas-related read errors.\n",
        "    \"\"\"\n",
        "    if not isinstance(file_path, str):\n",
        "        raise TypeError(\"file_path must be a string.\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Successfully loaded dataset from: {file_path}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error reading CSV file '{file_path}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pURC7rQz_4OG"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of the dataset\n",
        "# We verify the columns, data types, and ensure that our imputation worked\n",
        "# All columns should have non-null counts matching the total number of entries.\n",
        "def display_initial_info(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Displays the first few rows and basic info of a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to inspect.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If df is not a pandas DataFrame.\n",
        "        ValueError: If df is None.\n",
        "    \"\"\"\n",
        "    print(\"===============First 5 rows:===============\")\n",
        "    print(df.head())\n",
        "    print(\"\\n===============Information:===============\")\n",
        "    df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hkLorgFu_4OH"
      },
      "outputs": [],
      "source": [
        "def separate_features_target(df: pd.DataFrame, target_column: str) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Separates features and the target variable from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to separate.\n",
        "        target_column (str): The name of the target column.\n",
        "\n",
        "    Returns:\n",
        "        tuple[pd.DataFrame, pd.Series]: A tuple containing features (X) and target (y).\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If df is not a DataFrame or target_column is not a string.\n",
        "        ValueError: If df is None or target_column is not found in df.columns.\n",
        "    \"\"\"\n",
        "    if target_column not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame columns: {df.columns.tolist()}\")\n",
        "\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "    print(f\"\\nFeatures (X) and target (y, column: '{target_column}') have been separated.\")\n",
        "    print(f\"Shape of features (X): {X.shape}\")\n",
        "    print(f\"Shape of target (y): {y.shape}\")\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8YyZmr0d_4OI"
      },
      "outputs": [],
      "source": [
        "def split_data(X: pd.DataFrame, y: pd.Series,\n",
        "               test_size: float = 0.15,\n",
        "               val_relative_to_train_val_size: float = 0.15 / 0.85,\n",
        "               random_state: int = 42,\n",
        "               stratify_data: bool = True) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Splits feature and target data into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Features.\n",
        "        y (pd.Series): Target variable.\n",
        "        test_size (float): Proportion of the dataset to allocate to the test set.\n",
        "        val_relative_to_train_val_size (float): Proportion of the (train+validation) set to allocate to validation.\n",
        "        random_state (int): Seed for random number generator for reproducibility.\n",
        "        stratify_data (bool): Whether to stratify the split based on the target variable y.\n",
        "\n",
        "    Returns:\n",
        "        tuple: X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If X is not a DataFrame or y is not a Series.\n",
        "        ValueError: If X or y is None, or if shapes are incompatible.\n",
        "    \"\"\"\n",
        "    if len(X) != len(y):\n",
        "        raise ValueError(f\"Shape mismatch: X has {len(X)} samples, y has {len(y)} samples.\")\n",
        "\n",
        "    stratify_option_main = y if stratify_data else None\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=stratify_option_main\n",
        "    )\n",
        "\n",
        "    stratify_option_tv = y_train_val if stratify_data else None\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_relative_to_train_val_size,\n",
        "        random_state=random_state, stratify=stratify_option_tv\n",
        "    )\n",
        "\n",
        "    print(\"\\nData splitting completed.\")\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MJV8oSNG_4OJ"
      },
      "outputs": [],
      "source": [
        "def scale_features(X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame) -> tuple[StandardScaler, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Fits StandardScaler on X_train and transforms X_train, X_val, X_test.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Training features.\n",
        "        X_val (pd.DataFrame): Validation features.\n",
        "        X_test (pd.DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The fitted scaler object, and the scaled DataFrames (X_train_scaled, X_val_scaled, X_test_scaled).\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If any input is not a pandas DataFrame.\n",
        "        ValueError: If any input DataFrame is None.\n",
        "    \"\"\"\n",
        "    for df_name, df_obj in [(\"X_train\", X_train), (\"X_val\", X_val), (\"X_test\", X_test)]:\n",
        "        if not isinstance(df_obj, pd.DataFrame):\n",
        "            raise TypeError(f\"Input '{df_name}' must be a pandas DataFrame. Got {type(df_obj)}.\")\n",
        "        if df_obj is None:\n",
        "            raise ValueError(f\"Input DataFrame '{df_name}' cannot be None.\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    print(\"\\nFitting StandardScaler on X_train...\")\n",
        "    # Ensure X_train has data (not empty) before fitting\n",
        "    if X_train.empty:\n",
        "        raise ValueError(\"X_train is empty, cannot fit StandardScaler.\")\n",
        "    scaler.fit(X_train) # Fit ONLY on training data\n",
        "    print(\"Scaler fitted.\")\n",
        "\n",
        "    print(\"Transforming X_train, X_val, and X_test...\")\n",
        "    X_train_scaled_array = scaler.transform(X_train)\n",
        "    X_val_scaled_array = scaler.transform(X_val)\n",
        "    X_test_scaled_array = scaler.transform(X_test)\n",
        "    print(\"Transformation complete.\")\n",
        "\n",
        "    # Convert back to DataFrames\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=X_train.columns, index=X_train.index)\n",
        "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=X_val.columns, index=X_val.index)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=X_test.columns, index=X_test.index)\n",
        "    print(\"Scaled data converted back to DataFrames.\")\n",
        "\n",
        "    return scaler, X_train_scaled, X_val_scaled, X_test_scaled\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8VC-vsm_4OK"
      },
      "source": [
        "### 2.2 Feature Scaling Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm3oES5c_4OL",
        "outputId": "ee6938a3-d14d-4e42-ac68-7d44981dc20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded dataset from: imputed_water_potability_data.csv\n",
            "===============First 5 rows:===============\n",
            "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
            "0  7.036752  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
            "1  3.716080  129.422921  18630.057858     6.635246  333.073546    592.885359   \n",
            "2  8.099124  224.236259  19909.541732     9.275884  333.073546    418.606213   \n",
            "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
            "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
            "\n",
            "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
            "0       10.379783        86.990970   2.963135           0  \n",
            "1       15.180013        56.329076   4.500656           0  \n",
            "2       16.868637        66.420093   3.055934           0  \n",
            "3       18.436524       100.341674   4.628771           0  \n",
            "4       11.558279        31.997993   4.075075           0  \n",
            "\n",
            "===============Information:===============\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3276 entries, 0 to 3275\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ph               3276 non-null   float64\n",
            " 1   Hardness         3276 non-null   float64\n",
            " 2   Solids           3276 non-null   float64\n",
            " 3   Chloramines      3276 non-null   float64\n",
            " 4   Sulfate          3276 non-null   float64\n",
            " 5   Conductivity     3276 non-null   float64\n",
            " 6   Organic_carbon   3276 non-null   float64\n",
            " 7   Trihalomethanes  3276 non-null   float64\n",
            " 8   Turbidity        3276 non-null   float64\n",
            " 9   Potability       3276 non-null   int64  \n",
            "dtypes: float64(9), int64(1)\n",
            "memory usage: 256.1 KB\n"
          ]
        }
      ],
      "source": [
        "# Here, we load the dataset that has already undergone cleaning and imputation.\n",
        "# It's important to use the version where missing values have already been handled.\n",
        "# Path to the file which has already missing fields handled\n",
        "FILE_PATH_IMPUTED = \"imputed_water_potability_data.csv\"\n",
        "\n",
        "# Load the dataset\n",
        "df = load_data(FILE_PATH_IMPUTED)\n",
        "\n",
        "# Display the overview of the dataset\n",
        "display_initial_info(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpUnO5Dc_4OM",
        "outputId": "664d1b08-8715-4473-e0d7-cb408e1441c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Features (X) and target (y, column: 'Potability') have been separated.\n",
            "Shape of features (X): (3276, 9)\n",
            "Shape of target (y): (3276,)\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target variable\n",
        "X, y = separate_features_target(df, \"Potability\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfzFoiU-_4OM",
        "outputId": "5a9503d1-ea3f-47f9-959e-44b53c591790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data splitting completed.\n",
            "Shape of X_train: (2292, 9), y_train: (2292,)\n",
            "Shape of X_val: (492, 9), y_val: (492,)\n",
            "Shape of X_test: (492, 9), y_test: (492,)\n",
            "\n",
            "Proportion of samples in each set (approximate):\n",
            "Training set: 69.96%\n",
            "Validation set: 15.02%\n",
            "Test set: 15.02%\n"
          ]
        }
      ],
      "source": [
        "# Initialize split variables to None\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = [None] * 6\n",
        "\n",
        "if X is not None and y is not None:\n",
        "    try:\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, stratify_data=True)\n",
        "\n",
        "        # Display proportions\n",
        "        print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "        print(f\"Shape of X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "        print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "        total_samples = len(X)\n",
        "        print(\"\\nProportion of samples in each set (approximate):\")\n",
        "        print(f\"Training set: {len(X_train)/total_samples*100:.2f}%\")\n",
        "        print(f\"Validation set: {len(X_val)/total_samples*100:.2f}%\")\n",
        "        print(f\"Test set: {len(X_test)/total_samples*100:.2f}%\")\n",
        "\n",
        "    except TypeError as e:\n",
        "        raise ValueError(f\"TypeError during data splitting: {e}\")\n",
        "    except ValueError as e:\n",
        "        raise ValueError(f\"ValueError during data splitting: {e}\")\n",
        "else:\n",
        "    raise ValueError(\"X or y is None.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYGv07Nx_4ON",
        "outputId": "92b64c60-907f-4ed6-bcbb-0e75009182ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting StandardScaler on X_train...\n",
            "Scaler fitted.\n",
            "Transforming X_train, X_val, and X_test...\n",
            "Transformation complete.\n",
            "Scaled data converted back to DataFrames.\n",
            "\n",
            "--- Verification of Scaled Training Data ---\n",
            "===============First 5 rows:===============\n",
            "            ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
            "2766  0.292120  0.027566  0.522327     0.038376 -1.781081     -0.634797   \n",
            "2505 -0.104021  0.816164 -0.481655     0.540023 -0.954435      0.033630   \n",
            "163  -0.647064 -1.665957 -1.420733     0.308547 -1.555323     -0.140706   \n",
            "43    1.961394  0.193955 -1.379953    -0.160524  0.108557     -1.163968   \n",
            "2040 -0.022284  1.805507 -0.796542     0.846080  0.798238      0.565539   \n",
            "\n",
            "      Organic_carbon  Trihalomethanes  Turbidity  \n",
            "2766       -0.341967         0.209553  -0.180315  \n",
            "2505       -0.619904         0.161566  -1.631684  \n",
            "163         0.672902        -0.993148   0.866453  \n",
            "43          2.973166         0.352298   0.939613  \n",
            "2040        0.099535        -1.655482  -0.941722  \n",
            "\n",
            "===============Information:===============\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2292 entries, 2766 to 1559\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ph               2292 non-null   float64\n",
            " 1   Hardness         2292 non-null   float64\n",
            " 2   Solids           2292 non-null   float64\n",
            " 3   Chloramines      2292 non-null   float64\n",
            " 4   Sulfate          2292 non-null   float64\n",
            " 5   Conductivity     2292 non-null   float64\n",
            " 6   Organic_carbon   2292 non-null   float64\n",
            " 7   Trihalomethanes  2292 non-null   float64\n",
            " 8   Turbidity        2292 non-null   float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 179.1 KB\n",
            "\n",
            "Descriptive statistics of X_train_scaled (mean ~0, std ~1):\n",
            "            ph  Hardness   Solids  Chloramines  Sulfate  Conductivity  \\\n",
            "count  2292.00   2292.00  2292.00      2292.00  2292.00       2292.00   \n",
            "mean     -0.00      0.00     0.00        -0.00     0.00         -0.00   \n",
            "std       1.00      1.00     1.00         1.00     1.00          1.00   \n",
            "min      -4.19     -4.52    -2.48        -3.43    -4.29         -3.06   \n",
            "25%      -0.56     -0.58    -0.71        -0.63    -0.46         -0.75   \n",
            "50%      -0.02      0.02    -0.11         0.01    -0.01         -0.05   \n",
            "75%       0.54      0.61     0.60         0.62     0.44          0.69   \n",
            "max       4.45      3.84     4.46         3.72     4.13          4.09   \n",
            "\n",
            "       Organic_carbon  Trihalomethanes  Turbidity  \n",
            "count         2292.00          2292.00    2292.00  \n",
            "mean             0.00             0.00       0.00  \n",
            "std              1.00             1.00       1.00  \n",
            "min             -3.04            -3.70      -3.17  \n",
            "25%             -0.67            -0.62      -0.67  \n",
            "50%             -0.02             0.02      -0.00  \n",
            "75%              0.69             0.64       0.67  \n",
            "max              3.92             3.67       3.57  \n",
            "\n",
            "Workflow complete. Data is prepared for model training.\n",
            "Prepared data sets:\n",
            "X_train_scaled: (2292, 9), y_train: (2292,)\n",
            "X_val_scaled: (492, 9), y_val: (492,)\n",
            "X_test_scaled: (492, 9), y_test: (492,)\n"
          ]
        }
      ],
      "source": [
        "scaler_object, X_train_scaled, X_val_scaled, X_test_scaled = [None] * 4\n",
        "\n",
        "if X_train is not None and X_val is not None and X_test is not None:\n",
        "    try:\n",
        "        scaler_object, X_train_scaled, X_val_scaled, X_test_scaled = scale_features(X_train, X_val, X_test)\n",
        "\n",
        "        print(\"\\n--- Verification of Scaled Training Data ---\")\n",
        "        display_initial_info(X_train_scaled)\n",
        "\n",
        "        print(\"\\nDescriptive statistics of X_train_scaled (mean ~0, std ~1):\")\n",
        "        print(X_train_scaled.describe().round(2))\n",
        "\n",
        "        print(\"\\nWorkflow complete. Data is prepared for model training.\")\n",
        "        print(\"Prepared data sets:\")\n",
        "        print(f\"X_train_scaled: {X_train_scaled.shape}, y_train: {y_train.shape if y_train is not None else 'N/A'}\")\n",
        "        print(f\"X_val_scaled: {X_val_scaled.shape}, y_val: {y_val.shape if y_val is not None else 'N/A'}\")\n",
        "        print(f\"X_test_scaled: {X_test_scaled.shape}, y_test: {y_test.shape if y_test is not None else 'N/A'}\")\n",
        "\n",
        "    except (TypeError, ValueError) as e:\n",
        "        print(f\"Error during feature scaling: {e}\")\n",
        "else:\n",
        "    print(\"Skipping feature scaling as data splits are not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQtGxXth_4OO"
      },
      "source": [
        "## 3. Building and Training a Model\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3JDB5CPr_4OQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4uOzQLa_4OR",
        "outputId": "15813138-dfdd-4567-eed6-1f94c4071564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=l1(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=RMSprop(learning_rate=0.0005),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "wmGoEQXjBas5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "vfNtvqcPBePI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt6zdtpvBhKn",
        "outputId": "3f0e2041-3797-4868-b545-062e17777bd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5592 - loss: 3.7733 - val_accuracy: 0.6138 - val_loss: 3.2236\n",
            "Epoch 2/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5617 - loss: 3.0804 - val_accuracy: 0.6098 - val_loss: 2.6011\n",
            "Epoch 3/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5989 - loss: 2.4678 - val_accuracy: 0.6098 - val_loss: 2.0531\n",
            "Epoch 4/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6160 - loss: 1.9259 - val_accuracy: 0.6098 - val_loss: 1.5798\n",
            "Epoch 5/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6088 - loss: 1.4790 - val_accuracy: 0.6098 - val_loss: 1.1946\n",
            "Epoch 6/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6125 - loss: 1.1145 - val_accuracy: 0.6098 - val_loss: 0.9081\n",
            "Epoch 7/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6364 - loss: 0.8498 - val_accuracy: 0.6098 - val_loss: 0.7515\n",
            "Epoch 8/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5877 - loss: 0.7402 - val_accuracy: 0.6098 - val_loss: 0.6916\n",
            "Epoch 9/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6062 - loss: 0.6882 - val_accuracy: 0.6098 - val_loss: 0.6769\n",
            "Epoch 10/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6064 - loss: 0.6782 - val_accuracy: 0.6098 - val_loss: 0.6755\n",
            "Epoch 11/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6144 - loss: 0.6733 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 12/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6200 - loss: 0.6715 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 13/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6104 - loss: 0.6761 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 14/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6052 - loss: 0.6788 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 15/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6145 - loss: 0.6730 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 16/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6164 - loss: 0.6739 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 17/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6101 - loss: 0.6762 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 18/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6026 - loss: 0.6779 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 19/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5916 - loss: 0.6824 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 20/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6210 - loss: 0.6702 - val_accuracy: 0.6098 - val_loss: 0.6753\n",
            "Epoch 21/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6347 - loss: 0.6647 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 22/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6205 - loss: 0.6710 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 23/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6188 - loss: 0.6719 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 24/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6003 - loss: 0.6788 - val_accuracy: 0.6098 - val_loss: 0.6754\n",
            "Epoch 25/100\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6054 - loss: 0.6793 - val_accuracy: 0.6098 - val_loss: 0.6754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuAxnejVBkCM",
        "outputId": "466d1f8e-d869-4f99-f7ea-c08509d9ec7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Train Instance | Engineer Name | Regularizer | Optimizer | Early Stopping | Dropout Rate | Learning Rate | Accuracy | F1 Score | Recall | Precision |\n",
        "| -------------- | ------------- | ----------- | --------- | -------------- | ------------ | ------------- | -------- | -------- | ------ | --------- |\n",
        "| 2              | Nicholas EKE  | L1 (0.01)   | RMSprop   | val\\_loss, P=5 | 0.3          | 0.0005        | 0.61     | 0.00     | 0.00   | 0.00      |\n"
      ],
      "metadata": {
        "id": "lR1xMNpuDOhV"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}